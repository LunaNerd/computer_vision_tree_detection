{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "   _____                            _                __      ___     _                  _                        _ \n",
    "  / ____|                          | |               \\ \\    / (_)   (_)                | |                      | |\n",
    " | |     ___  _ __ ___  _ __  _   _| |_ ___ _ __      \\ \\  / / _ ___ _  ___  _ __      | |__   __ _ ___  ___  __| |\n",
    " | |    / _ \\| '_ ` _ \\| '_ \\| | | | __/ _ \\ '__|      \\ \\/ / | / __| |/ _ \\| '_ \\     | '_ \\ / _` / __|/ _ \\/ _` |\n",
    " | |___| (_) | | | | | | |_) | |_| | ||  __/ |          \\  /  | \\__ \\ | (_) | | | |    | |_) | (_| \\__ \\  __/ (_| |\n",
    "  \\_____\\___/|_| |_| |_| .__/ \\__,_|\\__\\___|_|           \\/   |_|___/_|\\___/|_| |_|    |_.__/ \\__,_|___/\\___|\\__,_|\n",
    "  _______              | | __  __                                                    _                             \n",
    " |__   __|             |_||  \\/  |                                                  | |                            \n",
    "    | |_ __ ___  ___      | \\  / | ___  __ _ ___ _   _ _ __ ___ _ __ ___   ___ _ __ | |_                           \n",
    "    | | '__/ _ \\/ _ \\     | |\\/| |/ _ \\/ _` / __| | | | '__/ _ \\ '_ ` _ \\ / _ \\ '_ \\| __|                          \n",
    "    | | | |  __/  __/     | |  | |  __/ (_| \\__ \\ |_| | | |  __/ | | | | |  __/ | | | |_                           \n",
    "    |_|_|  \\___|\\___|     |_|  |_|\\___|\\__,_|___/\\__,_|_|  \\___|_| |_| |_|\\___|_| |_|\\__|                          \n",
    "                                                                                                                   \n",
    "                                                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "## CONSTANTS\n",
    "\n",
    "# Assets folder\n",
    "ASSETS = \"./assets\"\n",
    "\n",
    "# Original video's\n",
    "CALIBRATION_VIDEO     = f\"{ASSETS}/original/computervisie_2024/calibration.MP4\"\n",
    "EASTBOUND_VIDEO       = f\"{ASSETS}/original/computervisie_2024/eastbound_20240319.MP4\"\n",
    "WESTBOUND_VIDEO       = f\"{ASSETS}/original/computervisie_2024/westbound_20240319.MP4\"\n",
    "\n",
    "\n",
    "# Extracted frames\n",
    "EXTRACTED_CALIBRATION_PATH  = f\"{ASSETS}/extracted_30/calibration/\"\n",
    "EXTRACTED_WESTBOUND_PATH    = f\"{ASSETS}/extracted_05/westbound_20240319/\"\n",
    "EXTRACTED_EASTBOUND_PATH    = f\"{ASSETS}/extracted_05/eastbound_20240319/\"\n",
    "EXTRACTED_CALIBRATION_FILES = f\"{EXTRACTED_CALIBRATION_PATH}*.png\"\n",
    "EXTRACTED_WESTBOUND_FILES   = f\"{EXTRACTED_WESTBOUND_PATH}*.png\"\n",
    "EXTRACTED_EASTBOUND_FILES   = f\"{EXTRACTED_EASTBOUND_PATH}*.png\"\n",
    "\n",
    "\n",
    "# Undistorted frames\n",
    "UNDISTORTED_WESTBOUND_PATH  = f\"{ASSETS}/undistorted_05/westbound/\"\n",
    "UNDISTORTED_EASTBOUND_PATH  = f\"{ASSETS}/undistorted_05/eastbound/\"\n",
    "UNDISTORTED_WESTBOUND_FILES = f\"{ASSETS}/undistorted_05/westbound/*.png\"\n",
    "UNDISTORTED_EASTBOUND_FILES = f\"{ASSETS}/undistorted_05/eastbound/*.png\"\n",
    "\n",
    "\n",
    "# Annotated frames\n",
    "ANNOTATED_WESTBOUND = f\"{ASSETS}/annotated_05/westbound/\"\n",
    "ANNOTATED_EASTBOUND = f\"{ASSETS}/annotated_05/eastbound/\"\n",
    "\n",
    "\n",
    "# PercepTree Model\n",
    "PERCEPTREE_MODEL = f\"{ASSETS}/models/ResNext-101_fold_01.pth\"\n",
    "\n",
    "\n",
    "# Colmap Workspaces\n",
    "COLMAP_WORKSPACE_WESTBOUND = f\"{ASSETS}/colmap/workspaces/westbound/\"\n",
    "COLMAP_WORKSPACE_EASTBOUND = f\"{ASSETS}/colmap/workspaces/eastbound/\"\n",
    "\n",
    "# temp local\n",
    "#COLMAP_WORKSPACE_EASTBOUND = f\"C:/2023-2024/M_ComputerVisie/colmap_sparse_and_dense_and_text/\"\n",
    "\n",
    "\n",
    "\n",
    "# Colmap TXT reconstructions\n",
    "COLMAP_TXT_RECON_EASTBOUND = f\"{ASSETS}/colmap/reconstruction/eastbound/\"\n",
    "COLMAP_TXT_RECON_WESTBOUND = f\"{ASSETS}/colmap/reconstruction/westbound/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install gdown\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "\n",
    "\n",
    "\n",
    "# When running code in JupyterHub / Google Colab, Opencv might not be\n",
    "# installed. This piece of code installs it when it is not yet available.\n",
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    print(\"OpenCV is not installed, installing now\")\n",
    "    !pip install opencv-python\n",
    "    import cv2\n",
    "\n",
    "from __future__ import  absolute_import\n",
    "\n",
    "\n",
    "is_linux = False\n",
    "if is_linux:\n",
    "    # detectron 2 install & imports\n",
    "    !pip install \"git+https://github.com/facebookresearch/detectron2.git\"\n",
    "    # NOTE: Doesn't seem to work on windows?\n",
    "\n",
    "    # Setup detectron2 logger\n",
    "    from detectron2.utils.logger import setup_logger\n",
    "    setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# general imports\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# matplotlib imports\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "\n",
    "# other visualisation tool imports\n",
    "# import open3d as o3d\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if is_linux:\n",
    "    # detectron2 utilities imports\n",
    "    from detectron2 import model_zoo\n",
    "    from detectron2.config import get_cfg\n",
    "    from detectron2.data import MetadataCatalog\n",
    "    from detectron2.engine import DefaultPredictor\n",
    "    from detectron2.utils.visualizer import Visualizer\n",
    "    from detectron2.utils.video_visualizer import VideoVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Source Video's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eastbound Video\n",
    "file_path = Path(EASTBOUND_VIDEO)\n",
    "\n",
    "# If video does not exist, we need to download the video's\n",
    "if not file_path.exists():\n",
    "    print(\"Downloading video's\")\n",
    "\n",
    "    url = \"https://telin.ugent.be/nextcloud/index.php/s/rjgf4cw7m2iTGbx/download\"\n",
    "\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Open a file in binary write mode\n",
    "        with open(f\"{ASSETS}/videos.zip\", \"wb\") as file:\n",
    "            # Iterate over the response content by chunks\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                # Write the chunk to the file\n",
    "                file.write(chunk)\n",
    "    \n",
    "        print(\"Zip file downloaded successfully.\")\n",
    "    \n",
    "        with ZipFile(f\"{ASSETS}/videos.zip\", \"r\") as zip_ref:\n",
    "            zip_ref.extractall(f\"{ASSETS}/original\")\n",
    "    else:\n",
    "        print(\"Failed to download the zip file.\")\n",
    "else:\n",
    "    print(\"Video's already exist, skipping download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download PercepTree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = Path(PERCEPTREE_MODEL)\n",
    "\n",
    "if not file_path.exists():\n",
    "    print('Downloading model')\n",
    "    !gdown --fuzzy 'https://drive.google.com/file/d/108tORWyD2BFFfO5kYim9jP0wIVNcw0OJ/view' -O assets/models/\n",
    "else:\n",
    "    print('Model already downloaded, skipping download')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download & Install Colmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "\n",
    "!sudo apt-get install \\\n",
    "    git \\\n",
    "    cmake \\\n",
    "    ninja-build \\\n",
    "    build-essential \\\n",
    "    libboost-program-options-dev \\\n",
    "    libboost-filesystem-dev \\\n",
    "    libboost-graph-dev \\\n",
    "    libboost-system-dev \\\n",
    "    libeigen3-dev \\\n",
    "    libflann-dev \\\n",
    "    libfreeimage-dev \\\n",
    "    libmetis-dev \\\n",
    "    libgoogle-glog-dev \\\n",
    "    libgtest-dev \\\n",
    "    libsqlite3-dev \\\n",
    "    libglew-dev \\\n",
    "    qtbase5-dev \\\n",
    "    libqt5opengl5-dev \\\n",
    "    libcgal-dev \\\n",
    "    libceres-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=compute_cap --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install gcc-10 g++-10 -y\n",
    "!export CC=/usr/bin/gcc-10\n",
    "!export CXX=/usr/bin/g++-10\n",
    "!export CUDAHOSTCXX=/usr/bin/g++-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/colmap/colmap.git ./assets/colmap/colmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda remove glog -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./assets/colmap/colmap_build\n",
    "!cd assets/colmap/colmap_build && ls\n",
    "!cd assets/colmap/colmap_build && cmake ../colmap -GNinja -DCMAKE_CUDA_ARCHITECTURES=61 -DGUI_ENABLED=OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd assets/colmap/colmap_build && ninja\n",
    "!cd assets/colmap/colmap_build && sudo ninja install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "## Frame Extraction\n",
    "\n",
    "In this section, we extract individual frames from the video. This process involves loading the video file, iterating through each $k$ frames, and saving these frames as separate image files. Each extracted frame is named to include its frame number, ensuring a clear and organized sequence. Extracting frames allows for detailed analysis and processing of each moment captured in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame extraction from video (keeping the original frame number)\n",
    "# this could help us in the case where we cannot detect trees in certain frames, then we can use the original frame number to extract more frames from the original video in this time area\n",
    "\n",
    "def extract_frames(video_path, capture_every_frame=5, output_folder=F\"{ASSETS}/extracted\"):\n",
    "    # Adjust the output folder to include how many frames were skipped\n",
    "    output_folder += f\"_{capture_every_frame:02d}\"\n",
    "\n",
    "    # Skip if video has already been extracted\n",
    "    if os.path.isdir(output_folder):\n",
    "        print(\"skipping, video already extracted\")\n",
    "        return\n",
    "    \n",
    "    # Extract video name from path\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    \n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(f\"{output_folder}/{video_name}\", exist_ok=True)\n",
    "\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get total number of frames\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total frames in {video_name}: {total_frames}\")\n",
    "\n",
    "    # Initialize the frame counter\n",
    "    current_frame = 0\n",
    "\n",
    "    # Process frames\n",
    "    while current_frame < total_frames:\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "        success, image = video.read()\n",
    "\n",
    "        # Check if the frame was successfully read\n",
    "        # sometimes this fails, corrupt frames in vide? idk\n",
    "        # using frames:05d (file_name_frame_00001.png) for easy sorting later on\n",
    "        if success:\n",
    "            # Save the frame with the actual frame number in the file name\n",
    "            cv2.imwrite(f\"{output_folder}/{video_name}/{video_name}_{current_frame:05d}.png\", image)\n",
    "            print(f\"{output_folder}/{video_name}/{video_name}_{current_frame:05d}.png\")\n",
    "            # Skip to the next frame based on the specified interval\n",
    "            current_frame += capture_every_frame\n",
    "        else:\n",
    "            # Output an error message if the frame failed to extract\n",
    "            print(f\"Frame {current_frame} failed to extract\")\n",
    "            # Try the next frame instead of skipping the interval because we couldn't read the current frame\n",
    "            current_frame += 1\n",
    "\n",
    "\n",
    "    # Release the video capture object\n",
    "    video.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames(CALIBRATION_VIDEO, capture_every_frame=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames(EASTBOUND_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_frames(WESTBOUND_VIDEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration\n",
    "\n",
    "In this section, we calculate the camera calibration matrices using Zhang's method, which is a widely used technique for camera calibration in computer vision. This method leverages multiple images of a known calibration pattern, in this case a chessboard, to estimate the camera's intrinsic and extrinsic parameters. By utilizing Zhang's method, this calibration process enables accurate determination of the camera's parameters, which are essential for correcting lens distortion and improving the accuracy of subsequent image processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(calibration_images):\n",
    "    objp = np.zeros((6*8, 3), np.float32)\n",
    "    objp[:, :2] = np.mgrid[0:8, 0:6].T.reshape(-1, 2)\n",
    "\n",
    "    objpoints = []\n",
    "    imgpoints = []\n",
    "\n",
    "\n",
    "    for img_path in calibration_images:\n",
    "        img = cv2.imread(img_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (8, 6), None)\n",
    "\n",
    "        if ret:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "    return ret, mtx, dist, rvecs, tvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_images = glob(EXTRACTED_CALIBRATION_FILES)\n",
    "\n",
    "mtx_file = f\"{ASSETS}/calibration_data/intrinsic_parameters.npy\"\n",
    "dist_file = f\"{ASSETS}/calibration_data/distortion_coefficients.npy\"\n",
    "rvecs_file = f\"{ASSETS}/calibration_data/rotation_vectors.npy\"\n",
    "tvecs_file = f\"{ASSETS}/calibration_data/translation_vectors.npy\"\n",
    "\n",
    "if os.path.isdir(f\"{ASSETS}/calibration_data\"):\n",
    "    print(\"skipping, callibration matrix already extracted\")\n",
    "\n",
    "    mtx = np.load(mtx_file)\n",
    "    dist = np.load(dist_file)\n",
    "    rvecs = np.load(rvecs_file)\n",
    "    tvecs = np.load(tvecs_file)\n",
    "\n",
    "else:\n",
    "    print(\"Calculating callibration data...\")\n",
    "    ret, mtx, dist, rvecs, tvecs = calibrate_camera(calibration_images)\n",
    "\n",
    "    # save data for later use\n",
    "    os.makedirs(f\"{ASSETS}/calibration_data\", exist_ok=True)\n",
    "    np.save(mtx_file, mtx)\n",
    "    np.save(dist_file, dist)\n",
    "    np.save(rvecs_file, rvecs)\n",
    "    np.save(tvecs_file, tvecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undistortion\n",
    "After calibrating the camera and obtaining the necessary calibration matrices, the next step is to undistort the images. Lens distortion, which manifests as warping or curving of straight lines in images, is a common issue in photography, especially with wide-angle lenses. Using the calibration data derived from Zhang's method, we can correct this radial distortion and produce geometrically accurate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort_images(images, output_loc=f\"{ASSETS}/undistorted\"):\n",
    "    os.makedirs(f\"{output_loc}\", exist_ok=True)\n",
    "\n",
    "    for img_path in images:\n",
    "        img = cv2.imread(img_path)\n",
    "        frame_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        \n",
    "        undistorted_image = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "        cv2.imwrite(f\"./{output_loc}/{frame_name}.png\", undistorted_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "westbound_images = glob(EXTRACTED_WESTBOUND_FILES)\n",
    "eastbound_images = glob(EXTRACTED_EASTBOUND_FILES)\n",
    "\n",
    "westbound_images.sort()\n",
    "eastbound_images.sort()\n",
    "\n",
    "undistort_images(westbound_images, output_loc=UNDISTORTED_WESTBOUND_PATH)\n",
    "undistort_images(eastbound_images, output_loc=UNDISTORTED_EASTBOUND_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Detection - TODO Clean Up\n",
    "\n",
    "## PercepTree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def detect_trees(base_path, output_dir, model_name=PERCEPTREE_MODEL, display_image=False ):\n",
    "    \n",
    "    # Ensure that the output directory exists, create it if necessary\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    image_paths = glob(image_dir_pattern)\n",
    "    image_paths.sort()\n",
    "    print(\"Images to process:\", len(image_paths))\n",
    "    \n",
    "    # def process_list_of_images():\n",
    "    torch.cuda.is_available()\n",
    "    logger = setup_logger(name=__name__)\n",
    "    \n",
    "    # All configurables are listed in /repos/detectron2/detectron2/config/defaults.py        \n",
    "    cfg = get_cfg()\n",
    "    cfg.INPUT.MASK_FORMAT = \"bitmask\"\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "    # cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))\n",
    "    # cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.DATASETS.TRAIN = ()\n",
    "    cfg.DATASETS.TEST = ()\n",
    "    cfg.DATALOADER.NUM_WORKERS = 8\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 8\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   # faster (default: 512)\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (tree)\n",
    "    cfg.MODEL.SEM_SEG_HEAD.NUM_CLASSES = 1  \n",
    "    cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 5\n",
    "    cfg.MODEL.MASK_ON = True\n",
    "    \n",
    "    cfg.OUTPUT_DIR = './output'\n",
    "    # cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, model_name)\n",
    "    cfg.MODEL.WEIGHTS = PERCEPTREE_MODEL\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7\n",
    "    # cfg.INPUT.MIN_SIZE_TEST = 0  # no resize at test time\n",
    "    \n",
    "    # set detector\n",
    "    predictor_synth = DefaultPredictor(cfg)    \n",
    "    \n",
    "    # set metadata\n",
    "    tree_metadata = MetadataCatalog.get(\"my_tree_dataset\").set(thing_classes=[\"Tree\"], keypoint_names=[\"kpCP\", \"kpL\", \"kpR\", \"AX1\", \"AX2\"])\n",
    "    \n",
    "    for image_path in image_paths:    \n",
    "        file_name = image_path.split(\"/\")[-1]\n",
    "        output_path = output_dir + file_name\n",
    "        # inference\n",
    "        im = cv2.imread(image_path)\n",
    "        outputs_pred = predictor_synth(im)\n",
    "        v_synth = Visualizer(im[:, :, ::-1],\n",
    "                        metadata=tree_metadata, \n",
    "                        scale=1,\n",
    "        )\n",
    "        predictions = outputs_pred[\"instances\"].to(\"cpu\")\n",
    "        out_synth = v_synth.draw_instance_predictions(predictions)\n",
    "    \n",
    "        # Assuming out_synth.get_image() returns the image\n",
    "        image = out_synth.get_image()[:, :, ::-1]  # Assuming the image is in BGR format, converting it to RGB\n",
    "        \n",
    "        # Save the image\n",
    "        cv2.imwrite(output_path, image)\n",
    "        print(\"Succesfully wrote image to:\", output_path)\n",
    "        \n",
    "        # Convert the tensors to lists\n",
    "        pred_boxes_list = predictions.get(\"pred_boxes\").tensor.tolist()\n",
    "        scores_list = predictions.get(\"scores\").tolist()\n",
    "        pred_keypoints_list = predictions.get(\"pred_keypoints\").tolist()\n",
    "        \n",
    "        # .tolist() on a tensor is extremely slow, so saved it as npy instead (you can test this in a seperate cell and see how slow it is)\n",
    "        # pred_masks_list = predictions.get(\"pred_masks_list\").tolist()\n",
    "        # pred_keypoints_heatmaps_list = predictions.get(\"pred_keypoint_heatmaps\").tolist()\n",
    "        pred_mask_numpy = predictions.get(\"pred_masks\").numpy()\n",
    "        pred_keypoint_heatmaps_numpy = predictions.get(\"pred_keypoint_heatmaps\").numpy()\n",
    "    \n",
    "        # File base name\n",
    "        file_base_name = file_name.split(\".png\")[0]\n",
    "        # File names are kept in json as reference to numpy array file\n",
    "        pred_masks_file_name = file_base_name + '_pred_mask.npy'\n",
    "        pred_keypoint_heatmaps_file_name = file_base_name + '_pred_keypoints_heatmaps.npy'\n",
    "        # Create full path that is used for saving the .npy files\n",
    "        pred_mask_numpy_path = output_dir + pred_masks_file_name\n",
    "        pred_keypoint_heatmaps_numpy_path = output_dir + pred_keypoint_heatmaps_file_name\n",
    "        # Save the .npy arrays\n",
    "        print(\"Saving:\", pred_mask_numpy_path)\n",
    "        np.save(pred_mask_numpy_path, pred_mask_numpy)\n",
    "        print(\"Saving:\", pred_keypoint_heatmaps_numpy_path)\n",
    "        np.save(pred_keypoint_heatmaps_numpy_path, pred_keypoint_heatmaps_numpy)\n",
    "    \n",
    "        # Prepare a dictionary for JSON serialization\n",
    "        json_data = {\n",
    "            \"pred_boxes\": pred_boxes_list,\n",
    "            \"scores\": scores_list,\n",
    "            \"pred_keypoints\": pred_keypoints_list,\n",
    "            \"pred_masks\": pred_masks_file_name,\n",
    "            \"pred_keypoint_heatmaps\": pred_keypoint_heatmaps_file_name,\n",
    "        }\n",
    "        \n",
    "        # Save to a JSON file    \n",
    "        file_base_name = file_name.split(\".png\")[0]\n",
    "        json_path = output_dir + file_base_name + '.json'\n",
    "        with open(json_path, 'w') as json_file:\n",
    "            json.dump(json_data, json_file, indent=4)\n",
    "        \n",
    "        print(f\"Data has been saved to {json_path}\")    \n",
    "    \n",
    "        if display_image:        \n",
    "            plt.figure(figsize=(20, 10))\n",
    "            plt.imshow(out_synth.get_image())\n",
    "            # plt.axis('off')  # Turn off axis\n",
    "            plt.show()\n",
    "    \n",
    "        collected_garbage = gc.collect()\n",
    "        print(\"Removed garbage:\", collected_garbage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_trees(UNDISTORTED_EASTBOUND_FILES, ANNOTATED_EASTBOUND)\n",
    "detect_trees(UNDISTORTED_WESTBOUND_FILES, ANNOTATED_WESTBOUND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Triangulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colmap_reconstruction(workspace, image_path):\n",
    "    !mkdir -p {workspace}\n",
    "    !colmap automatic_reconstructor --workspace_path {workspace} --image_path {image_path} --sparse 1 --dense 0 --single_camera 1\n",
    "\n",
    "def convert_to_txt(input, output):\n",
    "    #!mkdir -p {output}\n",
    "    !colmap model_converter --input_path {input} --output_path {output} --output_type TXT\n",
    "\n",
    "convert_to_txt(\"assets/colmap/workspaces/eastbound/sparse_aligned\", \"assets/colmap/reconstruction/eastbound/sparse_aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eastbound \n",
    "colmap_reconstruction(COLMAP_WORKSPACE_EASTBOUND, UNDISTORTED_EASTBOUND_PATH )\n",
    "convert_to_txt(COLMAP_WORKSPACE_EASTBOUND, COLMAP_TXT_RECON_EASTBOUND )\n",
    "\n",
    "# Westbound\n",
    "colmap_reconstruction(COLMAP_WORKSPACE_WESTBOUND, UNDISTORTED_WESTBOUND_PATH )\n",
    "convert_to_txt(COLMAP_WORKSPACE_WESTBOUND, COLMAP_TXT_RECON_WESTBOUND )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p ./assets/workspaces/eastbound\n",
    "!colmap automatic_reconstructor --workspace_path ./assets/workspaces/eastbound --image_path ./assets/undistorted_05/eastbound --sparse 1 --dense 0 --single_camera 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align model orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir assets\\colmap\\workspaces\\eastbound\\sparse_aligned\n",
    "\n",
    "!colmap model_orientation_aligner \\\n",
    "        --input_path assets/colmap/workspaces/eastbound/sparse/0 \\\n",
    "        --output_path assets/colmap/workspaces/eastbound/sparse_aligned \\\n",
    "        --image_path assets/undistorted_05_short \\\n",
    "        --method MANHATTAN-WORLD\n",
    "\n",
    "!colmap model_orientation_aligner --input_path assets/colmap/workspaces/eastbound/sparse/0 --output_path assets/colmap/workspaces/eastbound/sparse_aligned --image_path assets/undistorted_05_short --method MANHATTAN-WORLD\n",
    "\n",
    "# I tried the IMAGE-ORIENTATION method aswel but it doesn't work very well\n",
    "# Colmap has a weird quirk where the Y-axis points downwards. This is by design appearantly.\n",
    "# https://medium.com/red-buffer/mastering-3d-spaces-a-comprehensive-guide-to-coordinate-system-conversions-in-opencv-colmap-ef7a1b32f2df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to txt files (optional)\n",
    "!mkdir ../assets/workspace/sparse_aligned_man_txt\n",
    "!colmap model_converter --input_path $workspace/sparse_aligned_man --output_path $workspace/sparse_aligned_man_txt --output_type TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_txt('./assets/temp/sparse_aligned', './assets/temp/sparse_aligned_txt' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir assets/colmap/workspaces/eastbound/dense_aligned\n",
    "\n",
    "#assets\\colmap\\workspaces\\eastbound\\dense\n",
    "\n",
    "\"\"\"!colmap model_orientation_aligner \\\n",
    "        --input_path assets/colmap/workspaces/eastbound/dense \\\n",
    "        --output_path assets/colmap/workspaces/eastbound/dense_aligned \\\n",
    "        --image_path assets/undistorted_05 \\\n",
    "        --method MANHATTAN-WORLD\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(file_path):\n",
    "    images = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elements = line.split()\n",
    "            image_id = int(elements[0])\n",
    "            qw, qx, qy, qz = map(float, elements[1:5])\n",
    "            tx, ty, tz = map(float, elements[5:8])\n",
    "            camera_id = int(elements[8])\n",
    "            image_name = elements[9]\n",
    "\n",
    "            line = f.readline()\n",
    "            points2d = []\n",
    "            elements = line.split()\n",
    "            for i in range(0, len(elements), 3):\n",
    "                x, y, point3d_id = map(float, elements[i:i+3])\n",
    "                points2d.append([x, y, int(point3d_id)])\n",
    "            points2d = np.array(points2d)\n",
    "\n",
    "            images[image_name] = {\n",
    "                'image_id': image_id,\n",
    "                'qw': qw, 'qx': qx, 'qy': qy, 'qz': qz,\n",
    "                'tx': tx, 'ty': ty, 'tz': tz,\n",
    "                'camera_id': camera_id, 'image_name': image_name, 'points2d': points2d\n",
    "            }\n",
    "    return images\n",
    "\n",
    "def load_points3d(file_path):\n",
    "    points3d = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            elements = line.split()\n",
    "            point_id = int(elements[0])\n",
    "            xyz = np.array(elements[1:4], dtype=float)\n",
    "            rgb = np.array(elements[4:7], dtype=int)\n",
    "            error = float(elements[7])\n",
    "            track = np.array(elements[8:], dtype=int).reshape(-1, 2)\n",
    "            points3d[point_id] = {'xyz': xyz, 'rgb': rgb, 'error': error, 'track': track}\n",
    "    return points3d\n",
    "\n",
    "def find_corresponding_3d_points(image_data, keypoints_2d, points3d):\n",
    "    corresponding_3d_points = []\n",
    "    for kp in keypoints_2d:\n",
    "        x, y = kp\n",
    "        distances = np.linalg.norm(image_data['points2d'][:, :2] - np.array([x, y]), axis=1)\n",
    "        closest_point_idx = np.argmin(distances)\n",
    "        point2d_id = image_data['points2d'][closest_point_idx, 2]\n",
    "        if point2d_id != -1:\n",
    "            corresponding_3d_points.append(points3d[point2d_id]['xyz'])\n",
    "        else:\n",
    "            corresponding_3d_points.append(None)\n",
    "    return corresponding_3d_points\n",
    "\n",
    "def interpolate_missing_point(p1, p2):\n",
    "    if p1 is None and p2 is not None:\n",
    "        return p2\n",
    "    if p1 is not None and p2 is None:\n",
    "        return p1\n",
    "    if p1 is not None and p2 is not None:\n",
    "        return (np.array(p1) + np.array(p2)) / 2\n",
    "    return None\n",
    "\n",
    "def ensure_all_points_mapped(corresponding_3d_points):\n",
    "    n = len(corresponding_3d_points)\n",
    "    for i in range(n):\n",
    "        if corresponding_3d_points[i] is None:\n",
    "            left = None\n",
    "            right = None\n",
    "            if i > 0:\n",
    "                left = corresponding_3d_points[i-1]\n",
    "            if i < n-1:\n",
    "                right = corresponding_3d_points[i+1]\n",
    "            corresponding_3d_points[i] = interpolate_missing_point(left, right)\n",
    "    return corresponding_3d_points\n",
    "\n",
    "def calculate_distance_3d(p1, p2):\n",
    "    return np.linalg.norm(np.array(p1) - np.array(p2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_closest_points(points, target_point, k=7):\n",
    "    \"\"\"\n",
    "    Finds the k closest points to a target point.\n",
    "\n",
    "    Args:\n",
    "    points (list of tuples): List of 2d point (x, y, _) points.\n",
    "    target_point (tuple): The (x, y) coordinates of the target point.\n",
    "    k (int, optional): Number of closest points to find. Default is 7.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: The k closest points including their additional data.\n",
    "    \"\"\"\n",
    "    temp = [(x, y) for x, y, _ in points]\n",
    "\n",
    "    distances = np.linalg.norm(temp - target_point, axis=1)\n",
    "    \n",
    "    k_indices = np.argsort(distances)[:k]\n",
    "    \n",
    "    return points[k_indices]\n",
    "\n",
    "\n",
    "def get_closest_3d_points_for_felling_cut(points2d, points3d, felling_cut):\n",
    "    \"\"\"\n",
    "    Finds the closest 3D points to a felling cut based on 2D points.\n",
    "\n",
    "    Args:\n",
    "    points2d (list of tuples): List of (x, y, id) points.\n",
    "    points3d (list of tuples): List of 3D points corresponding to the ids in points2d.\n",
    "    felling_cut (tuple): The (x, y) coordinates of the felling cut.\n",
    "\n",
    "    Returns:\n",
    "    list of tuples: The 3D points closest to the felling cut.\n",
    "    \"\"\"\n",
    "    closest_points = k_closest_points(np.array(points2d), np.array(felling_cut))\n",
    "\n",
    "    closest_points_3d = []\n",
    "\n",
    "    for point in closest_points:\n",
    "        punt3d_id = point[2]\n",
    "\n",
    "        closest_points_3d.append(points3d[int(punt3d_id)])\n",
    "\n",
    "    return closest_points_3d\n",
    "\n",
    "\n",
    "def get_closest_3d_points_for_image(points2d, points3d, pred_keypoints):\n",
    "    \"\"\"\n",
    "    Finds the closest 3D points for each felling cut in image.\n",
    "\n",
    "    Args:\n",
    "    points2d (list of tuples): List of (x, y, id) points.\n",
    "    points3d (list of tuples): List of 3D points corresponding to the ids in points2d.\n",
    "    pred_keypoints (list of lists): List of predicted keypoints, each containing (x, y) coordinates.\n",
    "\n",
    "    Returns:\n",
    "    list of lists: Each inner list contains the 3D points closest to the corresponding felling cut.\n",
    "    \"\"\"\n",
    "    closest_points = []\n",
    "    \n",
    "    for treepoints in pred_keypoints:\n",
    "        felling_cut = treepoints[0]\n",
    "\n",
    "        felling_point = (felling_cut[0], felling_cut[1])\n",
    "\n",
    "        closest_points_3d = get_closest_3d_points_for_felling_cut(points2d, points3d, felling_point)\n",
    "\n",
    "        closest_points.append(closest_points_3d)\n",
    "\n",
    "    return closest_points\n",
    "\n",
    "\n",
    "def generate_random_rgb_values(num_values = 250):\n",
    "    \"\"\"\n",
    "    Generates an array of random RGB values.\n",
    "\n",
    "    Args:\n",
    "    num_values (int): The number of RGB values to generate.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array of shape (num_values, 3) with random RGB values between 0 and 1.\n",
    "    \"\"\"\n",
    "    rgb_values = np.random.rand(num_values, 3)\n",
    "    return rgb_values\n",
    "\n",
    "def is_eastbound(name):\n",
    "    \"\"\"\n",
    "    This function checks if the given string contains the substring \"east\".\n",
    "    \n",
    "    Parameters:\n",
    "    name (str): The string to be checked for the presence of \"east\".\n",
    "    \"\"\"\n",
    "    if \"eastbound\" in name:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_json_data(json_path):\n",
    "    \"\"\"\n",
    "    This function reads a JSON file from the specified path and returns the data as a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    json_path (str): The path to the JSON file to be read.\n",
    "    \n",
    "    Returns:\n",
    "    dict: The data contained in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_filtered_points(points2d):\n",
    "    \"\"\"\n",
    "    This function filters out 2D points that do not have a corresponding 3D point.\n",
    "    \n",
    "    It assumes that the third element in each point (i.e., point[2]) indicates whether \n",
    "    there is a corresponding 3D point. If point[2] is greater than -1, the point has \n",
    "    a corresponding 3D point and is included in the filtered list.\n",
    "\n",
    "    Parameters:\n",
    "    points2d (list of lists): A list of 2D points where each point is represented as \n",
    "                              a list with at least three elements.\n",
    "\n",
    "    Returns:\n",
    "    list of lists: A list of 2D points that have corresponding 3D points.\n",
    "    \"\"\"\n",
    "    filtered_points2d = []\n",
    "    for point in points2d:\n",
    "        if int(point[2]) > -1:\n",
    "            filtered_points2d.append(point)\n",
    "\n",
    "    return filtered_points2d\n",
    "\n",
    "def getNumber(x):\n",
    "    num_w_ext = x.split('_')[2]\n",
    "    num = num_w_ext.split('.')[0]\n",
    "    return num\n",
    "\n",
    "\n",
    "def take_median_of_closest_3d_points(closest_points, median_list):\n",
    "    \"\"\"\n",
    "    This function calculates the median of the closest 3D points for each set of points in the input list and \n",
    "    appends the median to a provided list.\n",
    "\n",
    "    Parameters:\n",
    "    closest_points (list of lists of dicts): A list where each element is a list of dictionaries. Each dictionary \n",
    "                                             represents a point and contains a key 'xyz' with the 3D coordinates.\n",
    "    median_list (list): A list to which the calculated median of the 3D points will be appended.\n",
    "    \"\"\"\n",
    "    for tree in closest_points:\n",
    "        # calculate\n",
    "        points_to_calculate = []\n",
    "        for point in tree:\n",
    "            points_to_calculate.append(point['xyz'])\n",
    "\n",
    "        mean_array = np.median(points_to_calculate, axis=0)\n",
    "        median_list.append(mean_array)\n",
    "\n",
    "def process_images_and_compute_felling_cut_median(files, images, points3d):\n",
    "    \"\"\"\n",
    "    This function processes a list of image files, computes the median of the closest 3D points to the felling cuts \n",
    "    for each image, and returns a list of these medians.\n",
    "\n",
    "    Parameters:\n",
    "    files (list of str): A list of image file names to be processed.\n",
    "    images (dict): A dictionary where keys are image paths and values are dictionaries containing 'points2d' \n",
    "                   corresponding to each image.\n",
    "\n",
    "    Returns:\n",
    "    list: A list containing the median of the closest 3D points for the felling cuts of each image.\n",
    "\n",
    "    Detailed Steps:\n",
    "    1. Initialize an empty list `median_all_felling_cuts_list` to store the medians.\n",
    "    2. Iterate over the sorted list of image files using `tqdm` for progress display.\n",
    "    3. For each image, replace the file extension '.png' with '.json' to get the corresponding JSON file name.\n",
    "    4. Determine if the image is eastbound or westbound using the `is_eastbound` function.\n",
    "    5. Load the JSON data containing predicted keypoints from the appropriate directory.\n",
    "    6. Retrieve the 2D points for the current image from the `images` dictionary.\n",
    "    7. Filter out 2D points that do not have corresponding 3D points using the `get_filtered_points` function.\n",
    "    8. Compute the closest 3D points to all felling cuts of the current image using the `get_closest_3d_points_for_image` function.\n",
    "    9. Calculate the median of these closest 3D points using the `take_median_of_closest_3d_points` function and append it to `median_all_felling_cuts_list`.\n",
    "    10. Return the list of medians.\n",
    "\n",
    "    Example:\n",
    "    >>> files = [\"image1.png\", \"image2.png\"]\n",
    "    >>> images = {\n",
    "    ...     \"eastbound/image1.png\": {'points2d': [[1, 2, 3], [4, 5, 6]]},\n",
    "    ...     \"westbound/image2.png\": {'points2d': [[7, 8, 9], [10, 11, 12]]}\n",
    "    ... }\n",
    "    >>> median_all_felling_cuts_list = process_images_and_compute_felling_cut_median(files, images)\n",
    "    >>> print(median_all_felling_cuts_list)\n",
    "    [array([...]), array([...])]\n",
    "    \"\"\"\n",
    "    median_all_felling_cuts_list = []\n",
    "\n",
    "    for i, image in enumerate(tqdm(sorted(files), desc=\"Processing images\")):\n",
    "        \n",
    "        # get pred keypoints\n",
    "        json_name = image.replace('.png', '.json')\n",
    "\n",
    "        if (is_eastbound(image)):\n",
    "            data = get_json_data(f\"{ANNOTATED_EASTBOUND}{json_name}\")\n",
    "\n",
    "            # get points2d for current image\n",
    "            # TODO: Something weird going on here.\n",
    "            #points2d = images[image]['points2d']\n",
    "            points2d = images[f\"eastbound/{image}\"]['points2d']\n",
    "        else:\n",
    "            data = get_json_data(f\"{ANNOTATED_WESTBOUND}{json_name}\")\n",
    "\n",
    "            # get points2d for current image\n",
    "            points2d = images[image]['points2d']\n",
    "        \n",
    "        filtered_points2d = get_filtered_points(points2d)\n",
    "\n",
    "        # get closest points to all felling cuts of this image\n",
    "        closest_3d_points = get_closest_3d_points_for_image(filtered_points2d, points3d, data[\"pred_keypoints\"])\n",
    "\n",
    "        # take median of all these points\n",
    "        take_median_of_closest_3d_points(closest_3d_points, median_all_felling_cuts_list)\n",
    "\n",
    "    return median_all_felling_cuts_list\n",
    "\n",
    "\n",
    "\n",
    "def convert_points_to_df(points):\n",
    "    X = [item[0] for item in points]\n",
    "    Y = [item[1] for item in points]\n",
    "    Z = [item[2] for item in points]\n",
    "\n",
    "    df = pd.DataFrame({'X': X, 'Y': Y, 'Z': Z})\n",
    "    return df\n",
    "\n",
    "\n",
    "def cluster_points(points, eps=0.1, min_samples=3, with_outliers=False):\n",
    "    \"\"\"\n",
    "    This function clusters 3D points using the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) \n",
    "    algorithm and returns the clustered points in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    points (list of lists or tuples): A list where each element is a list or tuple representing a 3D point with \n",
    "                                      three coordinates (X, Y, Z).\n",
    "    eps (float): The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "                 This is not a maximum bound on the distances of points within a cluster. Default is 0.1.\n",
    "    min_samples (int): The number of samples (or total weight) in a neighborhood for a point to be considered \n",
    "                       as a core point. This includes the point itself. Default is 3.\n",
    "    with_outliers (bool): If True, the function returns all points including outliers. If False, outliers are removed.\n",
    "                          Default is False.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the coordinates of the input points and their corresponding cluster labels.\n",
    "                      If `with_outliers` is False, the DataFrame will not include points classified as outliers.\n",
    "\n",
    "    Example:\n",
    "    >>> points = [(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12)]\n",
    "    >>> clustered_df = cluster_points(points, eps=0.5, min_samples=2, with_outliers=True)\n",
    "    >>> print(clustered_df)\n",
    "        X   Y   Z  Cluster\n",
    "    0   1   2   3        0\n",
    "    1   4   5   6        0\n",
    "    2   7   8   9        1\n",
    "    3  10  11  12       -1\n",
    "\n",
    "    Note:\n",
    "    - The `Cluster` column contains the cluster labels assigned by DBSCAN. \n",
    "    - Points labeled as -1 are considered outliers by the DBSCAN algorithm.\n",
    "    \"\"\"\n",
    "    dataframe = convert_points_to_df(points)\n",
    "    \n",
    "    X = dataframe[['X', 'Y', 'Z']]\n",
    "\n",
    "    print(X.head())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    X_scaled = X\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    clusters = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "    dataframe['Cluster'] = clusters\n",
    "\n",
    "    if(with_outliers):\n",
    "        return dataframe\n",
    "\n",
    "    # df_no_outlier = dataframe[dataframe['Cluster'] != -1]\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = load_images('./assets/temp/sparse_aligned_txt/images.txt')\n",
    "#points3d = load_points3d('./assets/temp/sparse_aligned_txt/points3D.txt')\n",
    "\n",
    "images = load_images(f\"{COLMAP_TXT_RECON_EASTBOUND}/sparse_aligned/images.txt\")\n",
    "points3d = load_points3d(f\"{COLMAP_TXT_RECON_EASTBOUND}/sparse_aligned/points3D.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell goes through all images\n",
    "#     takes for each image all trees, \n",
    "#         calculates for each tree the closest points to that felling cut\n",
    "#         map these closest points to the 3d space\n",
    "#         take the median of these 3 points\n",
    "# Cluster all the median 3d points\n",
    "\n",
    "\n",
    "#IMG_LOC  = \"./assets/temp/eastbound/*.png\"\n",
    "IMG_LOC  = \"assets/annotated_05/eastbound/*.png\"\n",
    "COLORS   = generate_random_rgb_values()\n",
    "\n",
    "file_paths = glob(IMG_LOC)\n",
    "\n",
    "files = [os.path.basename(file_path) for file_path in file_paths]\n",
    "\n",
    "sorted_files = sorted(files, key=lambda x: int(getNumber(x)))\n",
    "\n",
    "median_all_felling_cuts_list = process_images_and_compute_felling_cut_median(sorted_files[:150], images, points3d)\n",
    "\n",
    "df_clustered_points = cluster_points(median_all_felling_cuts_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_and_3D_point_list(image):\n",
    "    \"\"\"\n",
    "    Calculates 3D median point + clusterid of the felling cut of every tree on the image\n",
    "\n",
    "    Parameters:\n",
    "    image (str) : filename of an image that has been annotated (only filename, no path)\n",
    "\n",
    "    Returns:\n",
    "    List: List of DBScan id's (int) in the order of the trees in the json file\n",
    "    List: List of 3D median points in the order of the trees in the json file\n",
    "\n",
    "    Note:\n",
    "    DBScan id is -4 if no matching 3D median point is found for the felling cut\n",
    "    DBScan id is -1 if the 3D median point is not part of a cluster\n",
    "    \"\"\"\n",
    "    \n",
    "    number = getNumber(image)\n",
    "\n",
    "    json_name = image.replace('.png', '.json')\n",
    "\n",
    "    data = get_json_data(ANNOTATED_EASTBOUND + json_name)\n",
    "\n",
    "    pred_keypoints = data[\"pred_keypoints\"]\n",
    "\n",
    "    # get points2d\n",
    "    if f\"eastbound/{image}\" not in images.keys():\n",
    "        print(\"image not used in sparse map\")\n",
    "        return (None, None)\n",
    "            \n",
    "\n",
    "    points2d_img = images[f\"eastbound/{image}\"]     ## images?\n",
    "    #points2d_img = images[f\"{image}\"]     ## images?\n",
    "\n",
    "    points2d = points2d_img['points2d']\n",
    "\n",
    "    filtered_points2d = get_filtered_points(points2d)\n",
    "\n",
    "    # get closest points\n",
    "    closest_3d_points = get_closest_3d_points_for_image(filtered_points2d, points3d, pred_keypoints)\n",
    "\n",
    "    # TAKE MEDIAN/MEAN\n",
    "    id_list = []\n",
    "    point_3d_list = []\n",
    "\n",
    "    for index, tree in enumerate(closest_3d_points):\n",
    "        points_to_calculate = []\n",
    "\n",
    "        for point in tree:\n",
    "            points_to_calculate.append(point['xyz'])\n",
    "\n",
    "        mean_array = np.median(points_to_calculate, axis=0)\n",
    "        point_3d_list.append(mean_array)\n",
    "\n",
    "        filtered_df = df_clustered_points[(df_clustered_points['X'] == mean_array[0]) & (df_clustered_points['Y'] == mean_array[1]) & (df_clustered_points['Z'] == mean_array[2])]\n",
    "        \"\"\"round_nr = 0\n",
    "        x_eq = round(df_clustered_points['X'], round_nr) == round(mean_array[0], round_nr)\n",
    "        y_eq = round(df_clustered_points['Y'], round_nr) == round(mean_array[1], round_nr)\n",
    "        z_eq = round(df_clustered_points['Z'], round_nr) == round(mean_array[2], round_nr)\n",
    "        filtered_df = df_clustered_points[x_eq & y_eq & z_eq]\"\"\"\n",
    "\n",
    "        if filtered_df.shape[0] != 0:\n",
    "            cluster = filtered_df.iloc[0].Cluster\n",
    "            id_list.append(cluster)\n",
    "        else:\n",
    "            cluster = -4\n",
    "            id_list.append(cluster)\n",
    "\n",
    "    return id_list, point_3d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_normal_map_paths(file_name, workspace):\n",
    "    depth_map_path = f\"{workspace}dense/stereo/depth_maps/eastbound/{file_name}\"  # Change this to your actual file path\n",
    "    normal_map_path = f\"{workspace}dense/stereo/normal_maps/eastbound/{file_name}\"  # Change this to your actual file path\n",
    "    return depth_map_path, normal_map_path\n",
    "\n",
    "def get_depth_map(frame_number, workspace, orig_width=2704 , orig_height=1520, verbose=False):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses a depth image for a frame number.\n",
    "\n",
    "    Args:\n",
    "    frame_number (int) : current frame number\n",
    "    workspace (string) : path of current workspace\n",
    "    orig_width (int, opt) : width of undistorted frames\n",
    "    orig_height (int, opt) : height of undistorted frames\n",
    "\n",
    "    Details:\n",
    "    1. Create file paths for depth and normal maps, check if exists\n",
    "    2. Read depth map from png.geometric.bin file -> np.ndarray\n",
    "    3. Clip depth values with 5 and 95 percentile depth value\n",
    "    4. Resize depth image so depth image size is equal to the dimensions of the real image\n",
    "    5. Replace every pixel of image with (depth > 5 units OR depth = 0) with np.NaN value for better visualization\n",
    "\n",
    "    Returns:\n",
    "    ndarray with dims (height*, width*) filled with dtype np.float32.   (* from original image, not depth image)\n",
    "    \"\"\"\n",
    "    # Set the file paths for the depth and normal maps\n",
    "\n",
    "    # TODO: change this to a more standardized file_name?\n",
    "    file_name = f\"eastbound_20240319_{frame_number}.png.geometric.bin\"\n",
    "\n",
    "    depth_map_path, normal_map_path = get_depth_normal_map_paths(file_name, workspace)\n",
    "\n",
    "    # Check if files exist\n",
    "    if not os.path.exists(depth_map_path):\n",
    "        raise FileNotFoundError(f\"File not found: {depth_map_path}\")\n",
    "\n",
    "    if not os.path.exists(normal_map_path):\n",
    "        raise FileNotFoundError(f\"File not found: {normal_map_path}\")\n",
    "    \n",
    "    def read_array(path):\n",
    "        \"\"\"\n",
    "        Util function to extract depth or normal image from png.geometric.bin file\n",
    "        \"\"\"    \n",
    "        with open(path, \"rb\") as fid:\n",
    "            width, height, channels = np.genfromtxt(\n",
    "                fid, delimiter=\"&\", max_rows=1, usecols=(0, 1, 2), dtype=int\n",
    "            )\n",
    "            fid.seek(0)\n",
    "            num_delimiter = 0\n",
    "            byte = fid.read(1)\n",
    "            while True:\n",
    "                if byte == b\"&\":\n",
    "                    num_delimiter += 1\n",
    "                    if num_delimiter >= 3:\n",
    "                        break\n",
    "                byte = fid.read(1)\n",
    "            array = np.fromfile(fid, np.float32)\n",
    "        array = array.reshape((width, height, channels), order=\"F\")\n",
    "        return np.transpose(array, (1, 0, 2)).squeeze()\n",
    "\n",
    "    # Read depth and normal maps\n",
    "    depth_map = read_array(depth_map_path)\n",
    "    # normal_map = read_array(normal_map_path)\n",
    "\n",
    "    # Set the visualization parameters\n",
    "    min_depth_percentile = 5\n",
    "    max_depth_percentile = 95\n",
    "\n",
    "    # Process the depth map based on percentiles\n",
    "    min_depth, max_depth = np.percentile(depth_map, [min_depth_percentile, max_depth_percentile])\n",
    "    if verbose:\n",
    "        print(f\"min_depth: {min_depth}, max_depth: {max_depth}\")\n",
    "    depth_map[depth_map < min_depth] = min_depth\n",
    "    depth_map[depth_map > max_depth] = max_depth\n",
    "\n",
    "    # Resize the depth map to match the dimensions of the original image\n",
    "    # depth_map_resized = cv2.resize(depth_map, (original_image.shape[1], original_image.shape[0]))\n",
    "    depth_map_resized = cv2.resize(depth_map, (orig_width, orig_height), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    depth_map = depth_map_resized\n",
    "\n",
    "    # Remove values that are too far from the camera and also set 0 values to NaN\n",
    "    # Replace zeros with NaNs for better visualization\n",
    "    depth_map[depth_map == 0] = np.nan\n",
    "    depth_map[depth_map > 5] = np.nan\n",
    "    \n",
    "    return depth_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_3d_coordinates_simple_radial(x, y, depth, fx, cx, cy, k1):\n",
    "    \"\"\"\n",
    "    Convert pixel coordinates to world coordinates using the depth value and camera intrinsics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming distortion is negligible for simplicity.\n",
    "    X = (x - cx) * depth / fx\n",
    "    Y = (y - cy) * depth / fx\n",
    "    Z = depth\n",
    "\n",
    "    X = (x - cx) * depth / fx\n",
    "    Y = (y - cy) * depth / fx\n",
    "    Z = depth / (1 + k1 * depth)\n",
    "    # return X, Y, Z\n",
    "    return np.array([X, Y, Z])\n",
    "\n",
    "def compute_tree_trunk_width(left_point, right_point, depth_map, fx, cx, cy, k1, depthm_override=False, d1=None, d2=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns width of between two 2D points on the frame\n",
    "    \"\"\"\n",
    "    \n",
    "    x1, y1 = left_point\n",
    "    x2, y2 = right_point\n",
    "    \n",
    "    mid_point = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "    xm = mid_point[0]\n",
    "    ym = mid_point[1]\n",
    "    \n",
    "    depth1 = depth_map[y1, x1]\n",
    "    depth2 = depth_map[y2, x2]\n",
    "    depthm = depth_map[ym, xm]\n",
    "    if verbose:\n",
    "        print(depth1, depthm, depth2)\n",
    "    if depthm_override:\n",
    "        if d1 is not None:\n",
    "            depth1 = d1\n",
    "        else:\n",
    "            depth1 = depthm\n",
    "        if d2 is not None:\n",
    "            depth2 = d2\n",
    "        else:\n",
    "            depth2 = depthm   \n",
    "        if verbose:     \n",
    "            print(depth1, depthm, depth2)\n",
    "    # depthm = depth_map[700,1650]\n",
    "    # print(depth1, depth2, depthm)\n",
    "    \n",
    "    point1_3d = compute_3d_coordinates_simple_radial(x1, y1, depth1, fx, cx, cy, k1)\n",
    "    point2_3d = compute_3d_coordinates_simple_radial(x2, y2, depth2, fx, cx, cy, k1)\n",
    "    \n",
    "    if verbose:\n",
    "        print(point1_3d, point2_3d)\n",
    "        print(point1_3d - point2_3d)\n",
    "    \n",
    "    width = np.linalg.norm(point1_3d - point2_3d)\n",
    "    return width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_width_between_keypoints(current_tree_keypoints, depth_map, units_per_meter, fx, cx, cy, k1, visualize_data=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculates width of single perceptree tree on a frame in units and meters\n",
    "    Returns a dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    middle_y_value = (current_tree_keypoints[1][1] + current_tree_keypoints[2][1]) / 2\n",
    "    left_point = current_tree_keypoints[1][0], current_tree_keypoints[1][1]\n",
    "    right_point = current_tree_keypoints[2][0], current_tree_keypoints[2][1]   \n",
    "        \n",
    "    left_point = (int(current_tree_keypoints[1][0]), int(middle_y_value))\n",
    "    right_point = (int(current_tree_keypoints[2][0]), int(middle_y_value))\n",
    "    if verbose:\n",
    "        print(left_point, right_point)\n",
    "    \n",
    "    ### For visualisations, see tree_width_estimation notebook\n",
    "    # if visualize_data:        \n",
    "    #     depth_values, xm,y1 = extract_depth_along_Y_axis(depth_map, left_point, right_point)\n",
    "    #     plot_points_Y_axis(depth_values, xm,y1)\n",
    "        \n",
    "    #     depth_values = extract_depth_along_line(depth_map, left_point, right_point, 20)\n",
    "    #     plot_points(depth_values, middle_y_value, left_point[0])\n",
    "    \n",
    "    \n",
    "    width_units = compute_tree_trunk_width(left_point, right_point, depth_map, fx, cx, cy, k1)\n",
    "    width_cm = (width_units / units_per_meter) * 100\n",
    "    if verbose:\n",
    "        print(f\"The width of the tree trunk is: {width_units} units\")\n",
    "        print(f\"The width of the tree trunk is: {width_cm} centimeters\")\n",
    "\n",
    "    width_units_mid = compute_tree_trunk_width(left_point, right_point, depth_map, fx, cx, cy, k1, depthm_override=True)\n",
    "    width_cm_mid = (width_units_mid / units_per_meter) * 100\n",
    "    if verbose:\n",
    "        print(f\"The width of the tree trunk is: {width_units_mid} units - using mid depth\")\n",
    "        print(f\"The width of the tree trunk is: {width_cm_mid} centimeters - using mid depth\") \n",
    "    \n",
    "    row_data = {\n",
    "        'width_units': width_units,\n",
    "        'width_cm': width_cm,\n",
    "        'width_units_mid': width_units_mid,\n",
    "        'width_cm_mid': width_cm_mid\n",
    "    }\n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Creating a list of frame numbers that have been used when creating the dense/sparse model\n",
    "#\n",
    "\n",
    "# TODO: clean this up, this can be prettier code\n",
    "\n",
    "depth_dir_pattern = f\"{COLMAP_WORKSPACE_EASTBOUND}/dense/stereo/depth_maps/*/*.png.geometric.bin\"\n",
    "\n",
    "depth_paths = glob(depth_dir_pattern)\n",
    "depth_paths.sort()\n",
    "\n",
    "frame_numbers = []\n",
    "\n",
    "for i in range(len(depth_paths)):\n",
    "    dotsplit = depth_paths[i].split('.')\n",
    "    frame_number = dotsplit[-4].split('_')[-1]\n",
    "    frame_numbers.append(frame_number)\n",
    "\n",
    "print(frame_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Camera intrinsices hardcoded, link the camera intrinsics with the ones obtained before.\n",
    "\n",
    "# Camera intrinsics\n",
    "fx = 1463.2\n",
    "cx = 1352\n",
    "cy = 760\n",
    "k1 = 0.01502\n",
    "\n",
    "# TODO: create option here to do calibration manually? Display current calibration.\n",
    "\n",
    "unit_calc_frame_nr = \"07851\"\n",
    "workspace = COLMAP_WORKSPACE_EASTBOUND\n",
    "unit_calc_depth_map = get_depth_map(unit_calc_frame_nr, workspace)\n",
    "\n",
    "right_point_road = (1650,1000)\n",
    "left_point_road = (460,950)\n",
    "\n",
    "road_width = compute_tree_trunk_width(left_point_road, right_point_road, unit_calc_depth_map, fx, cx, cy, k1, depthm_override=False)\n",
    "print(f\"The width of the road is: {road_width} units\")\n",
    "print(f\"The width of the road is: 4m\")\n",
    "units_per_meter = road_width / 4\n",
    "\n",
    "print(f\"\\t-> units per meter: {units_per_meter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big for-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Loops through every frame with a depth map, calculates the widths of the trees in the frame, \n",
    "    matches trees on frame with trees in dbscan results and calculates median + std depth accross all frames.\n",
    "Result is saved in csv.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "\n",
    "width_measurements_file = 'tree_widths_3.csv'\n",
    "\n",
    "#if not os.path.isfile(width_measurements_file):\n",
    "if True:\n",
    "    # Note: Visualizing data is not implemented in the notebook, See tree_width_estimation for some visual examples of the workflow\n",
    "    visualize_data = False\n",
    "    verbose = False\n",
    "\n",
    "    tree_list = []\n",
    "\n",
    "    tree_id = 1\n",
    "\n",
    "    for i in tqdm(range(len(frame_numbers))):\n",
    "        depth_map = get_depth_map(frame_numbers[i], COLMAP_WORKSPACE_EASTBOUND)\n",
    "        if verbose:\n",
    "            print(\"Currently reading file:\")\n",
    "        json_name = f\"eastbound_20240319_{frame_numbers[i]}.json\"\n",
    "        js = f\"{ANNOTATED_EASTBOUND}{json_name}\"\n",
    "        # js = json_paths[i]\n",
    "\n",
    "        with open(js, 'r') as js:\n",
    "            data = json.load(js)\n",
    "            \n",
    "        # Sort the data by the x-coordinate of the left keypoint\n",
    "        sorted_indices = sorted(range(len(data['pred_boxes'])), key=lambda i: data['pred_keypoints'][i][1][0])\n",
    "        sorted_pred_keypoints = [data['pred_keypoints'][i] for i in sorted_indices]\n",
    "        if verbose:\n",
    "            print(sorted_pred_keypoints)\n",
    "        \n",
    "        #if visualize_data:     \n",
    "        #    visualize_depth_map(depth_map, frame_numbers[i], sorted_pred_keypoints)\n",
    "\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        id_list, point_3D_list = get_id_and_3D_point_list(json_name.replace('.json', '.png'))\n",
    "        sorted_id_list = [id_list[i] for i in sorted_indices]\n",
    "        sorted_point_3D_list = [point_3D_list[i] for i in sorted_indices]\n",
    "        #\n",
    "        #\n",
    "        # \n",
    "         \n",
    "        number_of_trees = len(sorted_pred_keypoints)\n",
    "        \n",
    "        for j in range(number_of_trees):        \n",
    "            tree_trunk_number = j + 1\n",
    "\n",
    "            width_row_data = calculate_width_between_keypoints(sorted_pred_keypoints[j], depth_map, units_per_meter, fx, cx, cy, k1, visualize_data=visualize_data, verbose=verbose)\n",
    "            \n",
    "            # Create a dictionary with the data\n",
    "            row_data = {\n",
    "                'tree_id': sorted_id_list[j],\n",
    "                'X' : sorted_point_3D_list[j][0],\n",
    "                'Y' : sorted_point_3D_list[j][1],\n",
    "                'Z' : sorted_point_3D_list[j][2],\n",
    "                #'json_name': json_name, # this is not necessary in end result\n",
    "                'frame_number': frame_numbers[i],\n",
    "                'number_of_trees': number_of_trees,\n",
    "                'tree_trunk_number': tree_trunk_number,\n",
    "            }\n",
    "            row_data.update(width_row_data)\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            tree_list.append(row_data)\n",
    "            \n",
    "            tree_id+=1\n",
    "            \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(tree_list)\n",
    "\n",
    "    df.to_csv(width_measurements_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(width_measurements_file)\n",
    "\n",
    "cols = ['width_cm_mid'] # one or more\n",
    "\n",
    "Q1 = df[cols].quantile(0.25)\n",
    "print(type(Q1))\n",
    "print(Q1.shape)\n",
    "Q3 = df[cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "inliers_df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "outliers_df = df[~((df[cols] > (Q1 - 1.5 * IQR)) |(df[cols] < (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "#df = inliers_df\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "#df.head(20)\n",
    "\n",
    "#df.groupby(['tree_id'], as_index=False).agg({'width_units':['mean','std'],'width_cm':['mean','std'],'width_units_mid':['mean','std'],'width_cm_mid':['mean','std']})\n",
    "g_df = df.groupby(['tree_id'], as_index=False).agg({'width_cm':['mean','std'],'width_cm_mid':['mean','std'], 'number_of_trees':['count']})\n",
    "\n",
    "#print(outliers_df.shape)\n",
    "#outliers_df\n",
    "\n",
    "#df[(df.width_cm_mid > 80) | (df.width_cm_mid < 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strict_df = df[(df.width_cm_mid < 100) & (df.width_cm_mid > 5)]\n",
    "\n",
    "strict_df = strict_df.groupby(['tree_id'], as_index=False).agg({'width_cm':['mean','std'],'width_cm_mid':['mean','std'], 'number_of_trees':['count']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = g_df.join(strict_df, on=\"tree_id\", lsuffix='_before', rsuffix='_after')[[\"tree_id\", \"width_cm_mid_before\", \"width_cm_mid_after\"]]\n",
    "\n",
    "compare[\"diff\"] = compare[\"width_cm_mid_after\"][\"std\"] - compare[\"width_cm_mid_before\"][\"std\"]\n",
    "print(sum(compare[\"diff\"].dropna()))\n",
    "compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered_points_no_outliers = df_clustered_points[df_clustered_points['Cluster'] != -1]\n",
    "\n",
    "df_map_to_process = df_clustered_points_no_outliers.copy()\n",
    "\n",
    "df_map_to_process['Y'] = 0\n",
    "\n",
    "df_map = df_map_to_process.groupby('Cluster').median().reset_index()\n",
    "\n",
    "df_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#files = os.listdir(ANNOTATED_EASTBOUND)\n",
    "\n",
    "files = [os.path.basename(x) for x in glob(f\"{ANNOTATED_EASTBOUND}*.png\")]\n",
    "\n",
    "sorted_files = sorted(files, key=lambda x: int(getNumber(x)))\n",
    "\n",
    "# Generate 250 random RGB values\n",
    "colors = generate_random_rgb_values(250)\n",
    "\n",
    "\n",
    "another_list = []\n",
    "\n",
    "for index, image in tqdm(enumerate(sorted_files[:20])):\n",
    "\n",
    "    number = getNumber(image)\n",
    "\n",
    "    mask_path = image.replace('.png', '_pred_mask.npy')\n",
    "\n",
    "    masks = np.load(f\"./assets/annotated_05/eastbound/{mask_path}\", allow_pickle=True)\n",
    "\n",
    "    json_name = image.replace('.png', '.json')\n",
    "\n",
    "    data = get_json_data(f\"./assets/annotated_05/eastbound/{json_name}\")\n",
    "    pred_keypoints = data[\"pred_keypoints\"]\n",
    "\n",
    "    # get points2d\n",
    "    points2d_img = images[f\"eastbound/{image}\"]\n",
    "    points2d = points2d_img['points2d']\n",
    "    filtered_points2d = get_filtered_points(points2d)\n",
    "\n",
    "    # get closest points\n",
    "    closest_3d_points = get_closest_3d_points_for_image(filtered_points2d, points3d, pred_keypoints)\n",
    "    \n",
    "\n",
    "    image_path = f\"./assets/undistorted_05/eastbound/{image}\"  # Update with the path to your image file\n",
    "\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # TAKE MEDIAN/MEAN\n",
    "    id_list = []\n",
    "\n",
    "    for index, tree in enumerate(closest_3d_points):\n",
    "        points_to_calculate = []\n",
    "\n",
    "        for point in tree:\n",
    "            points_to_calculate.append(point['xyz'])\n",
    "\n",
    "        mean_array = np.median(points_to_calculate, axis=0)\n",
    "\n",
    "        filtered_df = df_clustered_points[(df_clustered_points['X'] == mean_array[0]) & (df_clustered_points['Y'] == mean_array[1]) & (df_clustered_points['Z'] == mean_array[2])]\n",
    "\n",
    "        id_list.append(filtered_df.iloc[0].Cluster)\n",
    "        \n",
    "\n",
    "    for index, pred_box in enumerate(data[\"pred_boxes\"]):\n",
    "        if(id_list[index] != -1):\n",
    "            start_point = (int(pred_box[0]), int(pred_box[1]))\n",
    "            end_point = (int(pred_box[2]), int(pred_box[3]))\n",
    "            color = [int(c * 255) for c in COLORS[int(id_list[index]) % len(COLORS)]]\n",
    "            thickness = 5\n",
    "\n",
    "            cv2.rectangle(img, start_point, end_point, (color[2], color[1], color[0]) , thickness)\n",
    "\n",
    "            mask = masks[index] * 255\n",
    "            color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "            color_mask[:,:,0] = (mask * color[2] * 255).astype(np.uint8)\n",
    "            color_mask[:,:,1] = (mask * color[1] * 255).astype(np.uint8) \n",
    "            color_mask[:,:,2] = (mask * color[0] * 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "            # Apply the mask onto the image\n",
    "            img = cv2.addWeighted(img, 1, color_mask, 0.5, 0)\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text = str(int(id_list[index]))\n",
    "            text_size = cv2.getTextSize(text, font, 1, 2)[0]\n",
    "            text_x = int(pred_box[0] + (pred_box[2] - pred_box[0]) // 2 - text_size[0] // 2)\n",
    "            text_y = int( pred_box[1] + (pred_box[3] - pred_box[1]) // 2 + text_size[1] // 2)\n",
    "            \n",
    "            # Add a white background for the text\n",
    "            bg_top_left = (text_x - 5, text_y + 5)\n",
    "            bg_bottom_right = (text_x + text_size[0] + 5, text_y - text_size[1] - 5)\n",
    "            cv2.rectangle(img, bg_top_left, bg_bottom_right, (255, 255, 255), cv2.FILLED)\n",
    "\n",
    "            cv2.putText(img, text, (text_x, text_y), font, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    cv2.imwrite(f\"temp_output/output_{number}.png\", img)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    # Loop through each row in the median DataFrame and plot individually\n",
    "    for index, row in df_map.iterrows():\n",
    "        cluster = row['Cluster']\n",
    "        color = COLORS[int(cluster % len(COLORS))]\n",
    "        plt.scatter(row['Z'], -row['X'], color=color, s=100, label=f'Cluster {cluster}')\n",
    "\n",
    "\n",
    "    cam_x = points2d_img['tx']\n",
    "    cam_z = - points2d_img['tz']\n",
    "\n",
    "    plt.scatter(cam_x, cam_z, s=50, color=\"red\")\n",
    "    plt.xlim(cam_x -2, cam_x + 2)\n",
    "    plt.ylim(cam_z -2, cam_z + 2)\n",
    "\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig(f\"temp_plots/plot_image_{number}.png\")\n",
    "\n",
    "\n",
    "    plot = cv2.imread(f\"temp_plots/plot_image_{number}.png\", cv2.IMREAD_UNCHANGED)\n",
    "    tree_img = cv2.imread(f\"temp_output/output_{number}.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "\n",
    "    plot = plot[:, :, :3]\n",
    "\n",
    "    # Resize images to have the same height (assuming you want them to have the same height)\n",
    "    height = max(plot.shape[0], tree_img.shape[0])\n",
    "    plot_resized = cv2.resize(plot, (int(plot.shape[1] * height / plot.shape[0]), height))\n",
    "    tree_img_resized = cv2.resize(tree_img, (int(tree_img.shape[1] * height / tree_img.shape[0]), height))\n",
    "\n",
    "    # Concatenate images horizontally\n",
    "    final_image = np.concatenate((tree_img_resized, plot_resized), axis=1)\n",
    "\n",
    "    # Save or display the final image\n",
    "    cv2.imwrite(f\"testing/output_{number}.png\", final_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_images(image_folder, output_video, fps=30):\n",
    "    # Get all image file paths from the folder\n",
    "    image_paths = sorted(glob(os.path.join(image_folder)))  # Change '*.jpg' if your images have a different extension\n",
    "    if not image_paths:\n",
    "        print(\"No images found in the folder.\")\n",
    "        return\n",
    "\n",
    "    # Read the first image to get the frame size\n",
    "    frame = cv2.imread(image_paths[0])\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for mp4\n",
    "    video_writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        frame = cv2.imread(image_path)\n",
    "        video_writer.write(frame)  # Write the frame to the video\n",
    "\n",
    "    # Release the video writer object\n",
    "    video_writer.release()\n",
    "    print(f\"Video saved to {output_video}\")\n",
    "\n",
    "\n",
    "image_folder = './testing/*.png'\n",
    "output_video = 'output_video_eastbound_short.mp4'\n",
    "create_video_from_images(image_folder, output_video, fps=8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
